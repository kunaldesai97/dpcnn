{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, shutil\n",
    "import pickle\n",
    "import torch\n",
    "import torchtext\n",
    "import csv\n",
    "from core_dl.train_params import TrainParameters\n",
    "from trainbox import DPCNNTrainBox\n",
    "from core_dl.batch_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params = TrainParameters()\n",
    "test_params.DEV_IDS = [0]\n",
    "test_params.LOADER_NUM_THREADS = 0\n",
    "test_params.VALID_STEPS = 250\n",
    "test_params.MAX_VALID_BATCHES_NUM = 50\n",
    "test_params.VERBOSE_MODE = False\n",
    "test_params.LOADER_VALID_BATCH_SIZE = 85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Get Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model to source folder\n",
    "# https://drive.google.com/file/d/1-ap9imLHGnnvum30ekCLLThTH4zNj58S/view?usp=sharing\n",
    "checkpoint_dict = {'ckpt': r\"iter_034376.pth.tar\"}\n",
    "log_dir = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset (~2mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "data\\dbpedia_csv.tar.gz: 68.3MB [00:05, 11.8MB/s]\n",
      "560000lines [00:37, 14777.39lines/s]\n",
      "560000lines [00:59, 9401.45lines/s]\n",
      "70000lines [00:07, 9115.80lines/s]vocab len: 802999 label_len: 14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = \"data\"\n",
    "if not os.path.exists(data_path): \n",
    "    os.makedirs(data_path)\n",
    "\n",
    "_, test_dataset = torchtext.datasets.DBpedia(ngrams=1, root=data_path)\n",
    "\n",
    "vocab_len = len(test_dataset.get_vocab())\n",
    "label_len = len(test_dataset.get_labels())\n",
    "print(\"vocab len:\", vocab_len, \"label_len:\", label_len)\n",
    "\n",
    "sample_length = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m ## ┏━━ FORGIVE HAT ━━┓##\u001b[0m\n",
      " #    ┏┓     ┏┓  \n",
      " #   ┏┛┻━━━━━┛┻┓ \n",
      " #   ┃         ┃ \n",
      " #   ┃    ━    ┃ \n",
      " #   ┃ ┳┛   ┗┳ ┃ \n",
      " #   ┃         ┃ \n",
      " #   ┃    ┻    ┃ \n",
      " #   ┃         ┃ \n",
      " #   ┗━┓     ┏━┛ \n",
      " #     ┃     ┃   \n",
      " #     ┃     ┃   \n",
      " #     ┃     ┗━━━┓  \n",
      " #     ┃         ┣┓ \n",
      " #     ┃         ┏┛ \n",
      " #     ┗┓┓┏━━┳┓┏━┛  \n",
      " #      ┃┫┫  ┃┫┫    \n",
      " #      ┗┻┛  ┗┻┛    \n",
      "\n",
      "\u001b[42m ## This code is far away from bug with the animal protecting ##\u001b[0m\n",
      "[Training Parameters Overview] ------------------------------------------------------------------------\n",
      "dev_id :  [0]\n",
      "max_epochs :  4\n",
      "loader_batch_size :  6\n",
      "loader_valid_batch_size :  85\n",
      "loader_shuffle :  True\n",
      "start_learning_rate :  0.0001\n",
      "lr_decay_factor :  0.5\n",
      "lr_decay_epoch_size :  1\n",
      "loader_num_threads :  0\n",
      "verbose :  False\n",
      "valid_per_batches :  250\n",
      "valid_max_batch_num :  50\n",
      "checkpoint_per_iterations :  5000\n",
      "visualize_per_iterations :  100\n",
      "log_continue_step :  0\n",
      "description :  \n",
      "name_tag :  \n",
      "log_continue_dir :  \n",
      "[Optimizer Overview] ----------------------------------------------------------------------------------\n",
      "[<class 'torch.optim.rmsprop.RMSprop'>] Start learning rate: 0.010000\n",
      "Test Accuracy: 0.9774285714285714\n"
     ]
    }
   ],
   "source": [
    "# model hyper parameters\n",
    "channels = 128\n",
    "embed_dim = 128\n",
    "\n",
    "test_box = DPCNNTrainBox(train_params=test_params,\n",
    "                            vocab_size=vocab_len, \n",
    "                            label_size=label_len,\n",
    "                            text_length=sample_length, \n",
    "                            batchsize=test_params.LOADER_BATCH_SIZE,\n",
    "                            log_dir=log_dir,\n",
    "                            ckpt_path_dict=checkpoint_dict,\n",
    "                            channels=channels,\n",
    "                            embed_dim=embed_dim\n",
    "                          )\n",
    "\n",
    "\n",
    "acc, p, g = test_box.test_loop(test_dataset, generate_batch)\n",
    "print(\"Test Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li> We used a mini-batch RMSprop instead of SGD (as in the paper) since we obtained better results with RMSProp. </li>\n",
    "    <li> The number of epochs used in our experiments were 4-10 in contrast with the 30 epochs in the paper, we observed that our model was able to converge sooner and also get a good score. </li>\n",
    "    <li> We also used a learning rate decay factor of 0.1 after 8 epochs ((4/5)*number of epochs) as used in the paper.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis with Dbpedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the test accuracy and the training and validation accuracy plots below we see they all roughly converge to 98%. Not only is this a great score but it shows that the model is not overfitting or underfitting the data. Furthermore, the continuously decreasing validation loss also backup the claims of not overfitting the data. The proper fit of the model can be attributed to the architecture of the model, the normalization components of the model (drop out layer), the general training parameters and setup, and the L2 regularization (weight decay) during training.\n",
    "\n",
    "<img src=\"plots\\dbpedia-acc-train.png\" width=\"480\"> \n",
    "<img src=\"plots\\dbpedia-acc-valid.png\" width=\"480\">\n",
    "<img src=\"plots\\dbpedia-loss-valid.png\" width=\"480\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of predictions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. *The   James   Charnley   Residenceis located in Chicago’s Gold Coast neighborhoodin  the  1300  block  of  North  Astor  Street.    Thehouse is now called the Charnley–Persky Houseand  is  operated  as  a  museum  and  organizationheadquarters  by  The  Society  of  ArchitecturalHistorians (SAH). An Adler   Sullivan design thetownhouse  is  the  work  of  Louis  Sullivan  and  ayoung  Frank  Lloyd  Wright  who  was  a  juniordraftsman in Sullivan’s office at the time.*\n",
    "\n",
    "**Prediction**: 7 (Building)\n",
    "\\\n",
    "**Reference**: 7 (Building)\n",
    "\n",
    "\n",
    "2. *Film Magazine was a film weekly newsmagazine   published   in   Malayalam   Languagefrom Kerala India.  It was printed at Thiruvanan-thapuram and distributed throughout Kerala by Kalakaumudi publications private limited.  Even though  the  magazine  had  leniages  with  Kerala Kaumudi  news  paper,  it  was  an  independent company. It highlights the doings and happenings of the Mollywood film scene.*\n",
    "\n",
    "**Prediction**: 13 (Film)\n",
    "\\\n",
    "**Reference**: 14 (Written Work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of the model on other datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                     | DBpedia | AG   | Sogou | Yelp.p | Yelp.f | Yahoo | Ama.f | Ama.p |\n",
    "|---------------------|---------|------|-------|--------|--------|-------|-------|-------|\n",
    "| Training Accuracy   | 0.99    | 0.99 | 0.93  | 0.97   | 0.77   | 0.74  | 0.77  | 0.92  |\n",
    "| Validation Accuracy | 0.99    | 0.90 | 0.95  | 0.97   | 0.57   | 00.70 | 0.52  | 0.92  |\n",
    "| Test Accuracy       | 0.98    | 0.91 | 0.95  | 0.94   | 0.77   | 0.72  | 0.72  | 0.92  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
